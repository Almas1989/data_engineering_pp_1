"""DAG: raw_from_api_to_s3

Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Airflow DAG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸:
1. Ğ—Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸ÑÑ… Ñƒ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ USGS API Ğ·Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» (data_interval) Ğ¸
   Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº CSV.
2. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ raw-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² MinIO (ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ S3) Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet Ñ‡ĞµÑ€ĞµĞ· DuckDB.

Ğ¤Ğ°Ğ¹Ğ» Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Docker Compose (Ğ² ÑĞµÑ‚Ğ¸ `minio`) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ
Airflow Variable Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº S3 (`access_key`, `secret_key`).
"""

import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# --- ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ---
OWNER = "almas.maksutbekov"  # Ğ°Ğ²Ñ‚Ğ¾Ñ€/Ğ²Ğ»Ğ°Ğ´ĞµĞ»ĞµÑ† DAG
DAG_ID = "raw_from_api_to_s3"  # Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ DAG Ğ² Airflow

# Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑĞ»Ğ¾Ğ¹ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
LAYER = "raw"  # ÑĞ»Ğ¾Ğ¹ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² S3
SOURCE = "earthquake"  # Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

# S3 (MinIO) â€” Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºÑ€ĞµĞ´Ñ‹ Ğ¸Ğ· Airflow Variables
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² UI Airflow)
LONG_DESCRIPTION = """
# Ğ—Ğ°Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ DAG (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸)
"""

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° raw Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· USGS Ğ² S3 (MinIO) Ñ‡ĞµÑ€ĞµĞ· DuckDB"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 5, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ DAG Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Airflow.

    Airflow Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ `data_interval_start` Ğ¸ `data_interval_end` Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ `context`.
    Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² ÑÑ‚Ñ€Ğ¾ĞºÑƒ YYYY-MM-DD Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ‚ĞµĞ¶ (start_date, end_date).
    """
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")

    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ CSV Ğ¸Ğ· USGS API Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ² S3 (MinIO) Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet.

    Ğ¨Ğ°Ğ³Ğ¸:
    - Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ `start_date` Ğ¸ `end_date` Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· `get_dates`
    - Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº DuckDB (Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)
    - ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ `httpfs` Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ S3
    - Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº MinIO (endpoint, ĞºÑ€ĞµĞ´Ñ‹, ÑÑ‚Ğ¸Ğ»ÑŒ URL)
    - ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ CSV Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ HTTP-Ğ°Ğ´Ñ€ĞµÑĞ° USGS Ñ‡ĞµÑ€ĞµĞ· `read_csv_auto`
    - Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ `COPY ... TO 's3://.../...parquet'` â€” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½ Ğ² MinIO

    ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸Ğ´Ñ‘Ñ‚ Ğ² Ğ¿ÑƒÑ‚ÑŒ Ğ²Ğ¸Ğ´Ğ°
      s3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet
    Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ Ğ²Ğ°ÑˆĞ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.
    """

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑÑƒ DuckDB (Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)
    con = duckdb.connect()

    # Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº DuckDB: Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ httpfs Ğ¸ s3, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ CSV Ğ¿Ğ¾ HTTP Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² S3 ĞºĞ°Ğº parquet
    con.sql(
        f"""--sql
        SET TIMEZONE='UTC';
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{ACCESS_KEY}';
        SET s3_secret_access_key = '{SECRET_KEY}';
        SET s3_use_ssl = FALSE;

        COPY
        (
            SELECT
                *
            FROM
                read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}') AS res
        ) TO 's3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet';

        """,
    )

    con.close()
    logging.info(f"âœ… Download for date success: {start_date}")


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 5 * * *",  # Ğ·Ğ°Ğ¿ÑƒÑĞº ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾ Ğ² 05:00
    default_args=args,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:
    # ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ DAG Ğ² UI
    dag.doc_md = LONG_DESCRIPTION

    # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°: ÑÑ‚Ğ°Ñ€Ñ‚ -> Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° -> ĞºĞ¾Ğ½ĞµÑ†
    start = EmptyOperator(
        task_id="start",
    )

    # PythonOperator Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ²Ñ‹ÑˆĞµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
    get_and_transfer_api_data_to_s3 = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> get_and_transfer_api_data_to_s3 >> end